{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkuRnOixHjIhbqsjA26b38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GHBCOPS1/GHBCOPS1/blob/main/VCCpipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6158a294",
        "outputId": "c91523cb-958d-42b7-bc50-002d1839b153"
      },
      "source": [
        "!pip install scanpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.12.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5.2)\n",
            "Collecting legacy-api-wrap>=1.4.1 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.5)\n",
            "Requirement already satisfied: numba>=0.57.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (25.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.16.1)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.2.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.15.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.9.post2)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy)\n",
            "  Downloading zarr-3.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57.1->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->scanpy) (1.17.0)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numcodecs>=0.14 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (6.0.2)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Downloading scanpy-1.11.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.12.2-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading session_info2-0.2.1-py3-none-any.whl (16 kB)\n",
            "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.2-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: session-info2, numcodecs, legacy-api-wrap, donfig, crc32c, array-api-compat, zarr, anndata, scanpy\n",
            "Successfully installed anndata-0.12.2 array-api-compat-1.12.0 crc32c-2.7.1 donfig-0.8.1.post1 legacy-api-wrap-1.4.1 numcodecs-0.16.2 scanpy-1.11.4 session-info2-0.2.1 zarr-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvMfsNQDgpLR",
        "outputId": "acec3177-08bb-42a5-9f50-3e3401b5abac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Virtual Cell Challenge Analysis Pipeline\n",
            "Loading data from data/adata_Training.h5ad\n",
            "Data files not found: [Errno 2] Unable to synchronously open file (unable to open file: name = 'data/adata_Training.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Please ensure the Virtual Cell Challenge data is downloaded and paths are correct\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Virtual Cell Challenge Analysis Framework\n",
        "Integrates CNN, contextual neural networks, and AlphaFold3 predictions\n",
        "for comprehensive cellular perturbation analysis\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import scanpy as sc\n",
        "import anndata\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy import stats\n",
        "from scipy.sparse import csr_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import sqlite3\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    \"\"\"Configuration parameters for the analysis pipeline\"\"\"\n",
        "\n",
        "    # Data parameters\n",
        "    DATA_PATH = \"./data\"\n",
        "    OUTPUT_PATH = \"./output\"\n",
        "    N_GENES = 18080\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Model parameters\n",
        "    HIDDEN_DIMS = [512, 256, 128]\n",
        "    DROPOUT_RATE = 0.2\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Statistical parameters\n",
        "    ALPHA = 0.05\n",
        "    N_BOOTSTRAP = 1000\n",
        "    N_CLUSTERS = 5\n",
        "\n",
        "class VCCDataset(Dataset):\n",
        "    \"\"\"Dataset class for Virtual Cell Challenge data\"\"\"\n",
        "\n",
        "    def __init__(self, adata, perturbation_col='target_gene'):\n",
        "        self.X = torch.FloatTensor(adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X)\n",
        "        self.perturbations = adata.obs[perturbation_col].values\n",
        "        self.gene_names = adata.var_names.tolist()\n",
        "\n",
        "        # Create perturbation to index mapping\n",
        "        unique_perts = np.unique(self.perturbations)\n",
        "        self.pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
        "        self.pert_indices = torch.LongTensor([self.pert_to_idx[p] for p in self.perturbations])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'expression': self.X[idx],\n",
        "            'perturbation': self.pert_indices[idx],\n",
        "            'raw_expression': self.X[idx].clone()  # For prediction target\n",
        "        }\n",
        "\n",
        "class ContextualNN(nn.Module):\n",
        "    \"\"\"Contextual Neural Network for cell-type specific modeling\"\"\"\n",
        "\n",
        "    def __init__(self, n_genes, hidden_dims=[512, 256, 128], dropout_rate=0.2):\n",
        "        super(ContextualNN, self).__init__()\n",
        "\n",
        "        # Build network layers\n",
        "        layers = []\n",
        "        input_dim = n_genes\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.BatchNorm1d(hidden_dim)\n",
        "            ])\n",
        "            input_dim = hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(input_dim, n_genes))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class CNN1D(nn.Module):\n",
        "    \"\"\"1D CNN for capturing local gene expression patterns\"\"\"\n",
        "\n",
        "    def __init__(self, n_genes, kernel_sizes=[3, 5, 7], n_filters=64):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(1, n_filters, kernel_size=k, padding=k//2)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(n_filters * len(kernel_sizes), n_genes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape for 1D convolution: (batch, 1, genes)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Apply multiple convolutions\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_out = torch.relu(conv(x))\n",
        "            pooled = self.global_pool(conv_out).squeeze(-1)\n",
        "            conv_outputs.append(pooled)\n",
        "\n",
        "        # Concatenate and process\n",
        "        combined = torch.cat(conv_outputs, dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "class GeneEmbedding(nn.Module):\n",
        "    \"\"\"Gene embedding layer for perturbation-specific features\"\"\"\n",
        "\n",
        "    def __init__(self, n_perturbations, n_genes, embed_dim=128):\n",
        "        super(GeneEmbedding, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(n_perturbations, embed_dim)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim // 2, n_genes)\n",
        "        )\n",
        "\n",
        "    def forward(self, perturbation_indices):\n",
        "        embedded = self.embedding(perturbation_indices)\n",
        "        return self.projection(embedded)\n",
        "\n",
        "class VirtualCellPredictor(nn.Module):\n",
        "    \"\"\"Integrated model combining contextual, CNN, and embedding components\"\"\"\n",
        "\n",
        "    def __init__(self, n_genes, n_perturbations, hidden_dims=[512, 256, 128]):\n",
        "        super(VirtualCellPredictor, self).__init__()\n",
        "\n",
        "        self.contextual_encoder = ContextualNN(n_genes, hidden_dims)\n",
        "        self.cnn_features = CNN1D(n_genes)\n",
        "        self.embedding_layer = GeneEmbedding(n_perturbations, n_genes)\n",
        "\n",
        "        # Fusion layer with attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(n_genes, num_heads=8)\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(n_genes * 3, n_genes * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(n_genes * 2, n_genes)\n",
        "        )\n",
        "\n",
        "    def forward(self, expression, perturbation_indices):\n",
        "        # Extract features from different components\n",
        "        context_features = self.contextual_encoder(expression)\n",
        "        cnn_features = self.cnn_features(expression)\n",
        "        embed_features = self.embedding_layer(perturbation_indices)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        features = torch.stack([context_features, cnn_features, embed_features], dim=0)\n",
        "        attended_features, _ = self.attention(features, features, features)\n",
        "        attended_features = attended_features.mean(dim=0)  # Average across heads\n",
        "\n",
        "        # Fuse all features\n",
        "        all_features = torch.cat([context_features, cnn_features, embed_features], dim=1)\n",
        "        prediction = self.fusion(all_features) + attended_features  # Residual connection\n",
        "\n",
        "        return prediction\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Comprehensive evaluation framework with multiple metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def evaluate_predictions(self, y_true, y_pred, dataset_type=\"validation\"):\n",
        "        \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
        "\n",
        "        # Basic metrics\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        # Perturbation-specific metrics\n",
        "        perturbation_efficiency = self._calculate_perturbation_efficiency(y_true, y_pred)\n",
        "        gene_correlation = self._calculate_gene_correlations(y_true, y_pred)\n",
        "\n",
        "        metrics = {\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'r2': r2,\n",
        "            'perturbation_efficiency': perturbation_efficiency,\n",
        "            'mean_gene_correlation': np.mean(gene_correlation),\n",
        "            'dataset': dataset_type,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.metrics_history.append(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_perturbation_efficiency(self, y_true, y_pred, threshold=0.1):\n",
        "        \"\"\"Calculate perturbation efficiency\"\"\"\n",
        "        perturbation_mask = np.abs(y_true - y_pred) >= threshold\n",
        "        return np.mean(perturbation_mask)\n",
        "\n",
        "    def _calculate_gene_correlations(self, y_true, y_pred):\n",
        "        \"\"\"Calculate per-gene correlations\"\"\"\n",
        "        correlations = []\n",
        "        for i in range(y_true.shape[1]):\n",
        "            corr = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n",
        "            if not np.isnan(corr):\n",
        "                correlations.append(corr)\n",
        "        return correlations\n",
        "\n",
        "class StatisticalAnalyzer:\n",
        "    \"\"\"Statistical analysis framework\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def differential_analysis(self, metrics_df, control_col='perturbation', control_value='non-targeting'):\n",
        "        \"\"\"Perform differential analysis between conditions\"\"\"\n",
        "\n",
        "        results = []\n",
        "        control_mask = metrics_df[control_col] == control_value\n",
        "\n",
        "        for metric in ['mae', 'rmse', 'r2']:\n",
        "            if metric in metrics_df.columns:\n",
        "                # Mann-Whitney U test\n",
        "                control_data = metrics_df.loc[control_mask, metric].dropna()\n",
        "                treatment_data = metrics_df.loc[~control_mask, metric].dropna()\n",
        "\n",
        "                if len(control_data) > 0 and len(treatment_data) > 0:\n",
        "                    stat, p_value = stats.mannwhitneyu(\n",
        "                        treatment_data, control_data, alternative='two-sided'\n",
        "                    )\n",
        "\n",
        "                    # Effect size calculation\n",
        "                    n1, n2 = len(treatment_data), len(control_data)\n",
        "                    effect_size = 1 - (2 * stat) / (n1 * n2) if n1 * n2 > 0 else 0\n",
        "\n",
        "                    results.append({\n",
        "                        'metric': metric,\n",
        "                        'p_value': p_value,\n",
        "                        'effect_size': effect_size,\n",
        "                        'mean_treatment': treatment_data.mean(),\n",
        "                        'mean_control': control_data.mean()\n",
        "                    })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def bootstrap_confidence_intervals(self, data, n_bootstrap=1000, confidence=0.95):\n",
        "        \"\"\"Calculate bootstrap confidence intervals\"\"\"\n",
        "\n",
        "        bootstrap_samples = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(data, size=len(data), replace=True)\n",
        "            bootstrap_samples.append(np.mean(sample))\n",
        "\n",
        "        alpha = (1 - confidence) / 2\n",
        "        lower = np.percentile(bootstrap_samples, alpha * 100)\n",
        "        upper = np.percentile(bootstrap_samples, (1 - alpha) * 100)\n",
        "\n",
        "        return {\n",
        "            'mean': np.mean(bootstrap_samples),\n",
        "            'lower_ci': lower,\n",
        "            'upper_ci': upper,\n",
        "            'std': np.std(bootstrap_samples)\n",
        "        }\n",
        "\n",
        "class VCCPipeline:\n",
        "    \"\"\"Main pipeline for Virtual Cell Challenge analysis\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or Config()\n",
        "        self.model = None\n",
        "        self.evaluator = ComprehensiveEvaluator()\n",
        "        self.analyzer = StatisticalAnalyzer()\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(self.config.OUTPUT_PATH, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.config.OUTPUT_PATH, 'models'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.config.OUTPUT_PATH, 'results'), exist_ok=True)\n",
        "\n",
        "    def load_data(self, adata_path):\n",
        "        \"\"\"Load and prepare data\"\"\"\n",
        "        print(f\"Loading data from {adata_path}\")\n",
        "\n",
        "        if adata_path.endswith('.h5ad'):\n",
        "            adata = sc.read_h5ad(adata_path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "        # Basic preprocessing\n",
        "        sc.pp.filter_cells(adata, min_genes=200)\n",
        "        sc.pp.filter_genes(adata, min_cells=3)\n",
        "\n",
        "        return adata\n",
        "\n",
        "    def train_model(self, train_data, val_data=None, epochs=100):\n",
        "        \"\"\"Train the integrated model\"\"\"\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = VCCDataset(train_data)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "        if val_data is not None:\n",
        "            val_dataset = VCCDataset(val_data)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=self.config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        n_perturbations = len(np.unique(train_data.obs['target_gene']))\n",
        "        self.model = VirtualCellPredictor(\n",
        "            n_genes=self.config.N_GENES,\n",
        "            n_perturbations=n_perturbations,\n",
        "            hidden_dims=self.config.HIDDEN_DIMS\n",
        "        )\n",
        "\n",
        "        # Training setup\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.LEARNING_RATE)\n",
        "        criterion = nn.MSELoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            epoch_train_loss = 0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                predictions = self.model(batch['expression'], batch['perturbation'])\n",
        "                loss = criterion(predictions, batch['raw_expression'])\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Validation\n",
        "            if val_data is not None:\n",
        "                self.model.eval()\n",
        "                epoch_val_loss = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        predictions = self.model(batch['expression'], batch['perturbation'])\n",
        "                        loss = criterion(predictions, batch['raw_expression'])\n",
        "                        epoch_val_loss += loss.item()\n",
        "\n",
        "                avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "                val_losses.append(avg_val_loss)\n",
        "\n",
        "                scheduler.step(avg_val_loss)\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "            else:\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Save model\n",
        "        model_path = os.path.join(self.config.OUTPUT_PATH, 'models', 'vcc_model.pth')\n",
        "        torch.save(self.model.state_dict(), model_path)\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "        return {'train_losses': train_losses, 'val_losses': val_losses}\n",
        "\n",
        "    def evaluate_model(self, test_data, dataset_type=\"test\"):\n",
        "        \"\"\"Evaluate the trained model\"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained first\")\n",
        "\n",
        "        # Create test dataset\n",
        "        test_dataset = VCCDataset(test_data)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Make predictions\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                predictions = self.model(batch['expression'], batch['perturbation'])\n",
        "                all_predictions.append(predictions.cpu().numpy())\n",
        "                all_targets.append(batch['raw_expression'].cpu().numpy())\n",
        "\n",
        "        # Concatenate results\n",
        "        y_pred = np.vstack(all_predictions)\n",
        "        y_true = np.vstack(all_targets)\n",
        "\n",
        "        # Evaluate\n",
        "        metrics = self.evaluator.evaluate_predictions(y_true, y_pred, dataset_type)\n",
        "\n",
        "        # Statistical analysis\n",
        "        results_df = pd.DataFrame({\n",
        "            'gene': test_data.var_names.tolist() * len(test_data),\n",
        "            'perturbation': np.repeat(test_data.obs['target_gene'].values, len(test_data.var_names)),\n",
        "            'mae': np.abs(y_true - y_pred).flatten(),\n",
        "            'squared_error': ((y_true - y_pred) ** 2).flatten()\n",
        "        })\n",
        "\n",
        "        diff_analysis = self.analyzer.differential_analysis(results_df)\n",
        "\n",
        "        return {\n",
        "            'metrics': metrics,\n",
        "            'predictions': y_pred,\n",
        "            'targets': y_true,\n",
        "            'differential_analysis': diff_analysis,\n",
        "            'results_df': results_df\n",
        "        }\n",
        "\n",
        "    def run_complete_analysis(self, train_path, val_path=None, test_path=None):\n",
        "        \"\"\"Run complete analysis pipeline\"\"\"\n",
        "\n",
        "        print(\"Starting Virtual Cell Challenge Analysis Pipeline\")\n",
        "\n",
        "        # Load data\n",
        "        train_data = self.load_data(train_path)\n",
        "        val_data = self.load_data(val_path) if val_path else None\n",
        "        test_data = self.load_data(test_path) if test_path else None\n",
        "\n",
        "        # Train model\n",
        "        training_history = self.train_model(train_data, val_data)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        if test_data is not None:\n",
        "            test_results = self.evaluate_model(test_data, \"test\")\n",
        "\n",
        "            # Save results\n",
        "            results_path = os.path.join(self.config.OUTPUT_PATH, 'results', 'test_results.json')\n",
        "            with open(results_path, 'w') as f:\n",
        "                json.dump({\n",
        "                    'metrics': test_results['metrics'],\n",
        "                    'training_history': training_history\n",
        "                }, f, indent=2, default=str)\n",
        "\n",
        "            print(f\"Analysis complete. Results saved to {results_path}\")\n",
        "            print(f\"Test MAE: {test_results['metrics']['mae']:.4f}\")\n",
        "            print(f\"Test R²: {test_results['metrics']['r2']:.4f}\")\n",
        "\n",
        "            return test_results\n",
        "\n",
        "        return training_history\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Initialize pipeline\n",
        "    config = Config()\n",
        "    pipeline = VCCPipeline(config)\n",
        "\n",
        "    # Example usage (paths would need to be updated with actual data)\n",
        "    try:\n",
        "        results = pipeline.run_complete_analysis(\n",
        "            train_path=\"data/adata_Training.h5ad\",\n",
        "            val_path=\"data/adata_Validation.h5ad\",\n",
        "            test_path=\"data/adata_Test.h5ad\"\n",
        "        )\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        return results\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Data files not found: {e}\")\n",
        "        print(\"Please ensure the Virtual Cell Challenge data is downloaded and paths are correct\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "\n",
        "    # !pip install cell-eval\n",
        "    # run script through cell-eval script and save to output_path\n",
        "\n",
        "    # install from pypi\n",
        "    # uv pip install -U cell-eval\n",
        "\n",
        "    # install from github directly\n",
        "    # uv pip install -U git+ssh://github.com/arcinstitute/cell-eval\n",
        "\n",
        "    # install cli with uv tool\n",
        "    # uv tool install -U git+ssh://github.com/arcinstitute/cell-eval\n",
        "\n",
        "    # Check installation\n",
        "    # cell-eval --help\n",
        "    # Usage\n",
        "\n",
        "    # get(adata_pred)\n",
        "    # get((adata_real)\n",
        "\n",
        "    # prep (VCC)\n",
        "    # cell-eval prep\n",
        "\n",
        "    # Run predicted data\n",
        "    # cell-eval prep \\\n",
        "    #     -i <your/path/to>.h5ad \\\n",
        "    #         -g <expected_genelist>\n",
        "\n",
        "    # Run\n",
        "    # cell-eval run\n",
        "\n",
        "    # Run Differential expression for each anndata\n",
        "    # --profile flag\n",
        "\n",
        "    # submit result\n",
        "    # cell-eval run --help\n",
        "\n",
        "    # cell-eval run \\\n",
        "    #     -ap <your/path/to/pred>.h5ad \\\n",
        "    #         -ar <your/path/to/real>.h5ad \\\n",
        "    #             --num-threads 64 \\\n",
        "    #                 --profile full\n",
        "\n",
        "    # MetricsEvaluator class.\n",
        "    # from cell_eval import MetricsEvaluator\n",
        "    # from cell_eval.data import build_random_anndata, downsample_cells\n",
        "\n",
        "    # adata_real = build_random_anndata()\n",
        "    # adata_pred = downsample_cells(adata_real, fraction=0.5)\n",
        "    # evaluator = MetricsEvaluator(\n",
        "    #     adata_pred=adata_pred,\n",
        "    #     adata_real=adata_real,\n",
        "    #     control_pert=\"control\",\n",
        "    #     pert_col=\"perturbation\",\n",
        "    #     num_threads=64,\n",
        "    # )\n",
        "    # (results, agg_results) = evaluator.compute()\n",
        "\n",
        "    # (results)\n",
        "    # (agg_results)\n",
        "\n",
        "    # Score\n",
        "    # cell-eval score\n",
        "    # agg_results.csv (or agg_results)\n",
        "\n",
        "    # cell-eval score \\\n",
        "    #     --user-input <your/path/to/user>/agg_results.csv \\\n",
        "    #         --base-input <your/path/to/base>/agg_results.csv\n",
        "\n",
        "    # or\n",
        "\n",
        "    # from cell_eval import score_agg_metrics\n",
        "\n",
        "    # user_input = \"./cell-eval-user/agg_results.csv\"\n",
        "    # base_input = \"./cell-eval-base/agg_results.csv\"\n",
        "    # output_path = \"./score.csv\"\n",
        "\n",
        "    # score_agg_metrics(\n",
        "    #     results_user=user_input,\n",
        "    #     results_base=base_input,\n",
        "    #     output=output_path,\n",
        "    # )\n",
        "    # cell_eval.metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "output_path = \"./score.csv\"\n",
        "try:\n",
        "    score_df = pd.read_csv(output_path)\n",
        "    display(score_df)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Score file not found at {output_path}. Please ensure cell-eval run and cell-eval score were executed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zd1cN8kpWB3",
        "outputId": "08efee2d-5a71-4b0c-fe2e-5c5877f07a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score file not found at ./score.csv. Please ensure cell-eval run and cell-eval score were executed.\n"
          ]
        }
      ]
    }
  ]
}