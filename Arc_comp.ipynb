{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GHBCOPS1/GHBCOPS1/blob/main/Arc_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LGxhECkf2Wr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xqqY6Zf6mc"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq-goeWugADD"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm.auto import tqdm # picks the best bar for the environment\n",
        "\n",
        "url = \"https://storage.googleapis.com/vcc_data_prod/datasets/state/competition_support_set.zip\"\n",
        "output_path = \"competition_support_set.zip\"\n",
        "\n",
        "# stream the download so we can\n",
        "# track progress\n",
        "response = requests.get(url,\n",
        "                        stream=True)\n",
        "total = \\\n",
        "    int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "with open(output_path, \"wb\") as f, \\\n",
        "    tqdm(\n",
        "        total=total, unit='B',\n",
        "        unit_scale=True, desc=\"Downloading\"\n",
        "    ) as bar:\n",
        "    for chunk in \\\n",
        "            response.iter_content(chunk_size=8192):\n",
        "        if not chunk:\n",
        "            break\n",
        "        f.write(chunk)\n",
        "        bar.update(len(chunk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBfBJ2m0iUqe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ee99LFxf5R-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd970059"
      },
      "source": [
        "# Task\n",
        "Download and analyze the datasets from \"https://plus.figshare.com/articles/dataset/_Mapping_information-rich_genotype-phenotype_landscapes_with_genome-scale_Perturb-seq_Replogle_et_al_2022_processed_Perturb-seq_datasets/20029387\" using the analysis code from \"https://colab.research.google.com/drive/1QKOtYP7bMpdgDJEipDxaJqOchv7oQ-_l\" and \"https://colab.research.google.com/drive/1QKOtYP7bMpdgDJEipDxaJqOchv7oQ-_l#scrollTo=ohFQvuBY4dPr\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0d8ad2"
      },
      "source": [
        "## Download datasets\n",
        "\n",
        "### Subtask:\n",
        "Download the datasets from the provided URL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340b02d5"
      },
      "source": [
        "## Extract datasets\n",
        "\n",
        "### Subtask:\n",
        "Extract the contents of the downloaded zip file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ddeee1"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the contents of the downloaded zip file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bef7f6d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "extracted_files = os.listdir('.')\n",
        "print(\"Files in the extracted directory:\")\n",
        "for file in extracted_files:\n",
        "    print(file)\n",
        "\n",
        "# Assuming one of the extracted files is a data file (e.g., a CSV),\n",
        "# replace 'your_data_file.csv' with the actual filename and inspect it.\n",
        "# You might need to adjust the encoding based on the file.\n",
        "# try:\n",
        "#     with open('your_data_file.csv', 'r', encoding='utf-8') as f:\n",
        "#         print(\"\\nFirst 5 lines of your_data_file.csv:\")\n",
        "#         for i in range(5):\n",
        "#             print(f.readline())\n",
        "# except FileNotFoundError:\n",
        "#     print(\"\\nData file not found. Please replace 'your_data_file.csv' with the correct filename.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nError reading file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4c8444"
      },
      "source": [
        "# Task\n",
        "Download and analyze the datasets from \"https://plus.figshare.com/articles/dataset/_Mapping_information-rich_genotype-phenotype_landscapes_with_genome-scale_Perturb-seq_Replogle_et_al_2022_processed_Perturb-seq_datasets/20029387\" using the analysis code from \"https://colab.research.google.com/drive/1QKOtYP7bMpdgDJEipDxaJqOchv7oQ-_l\" and \"https://colab.research.google.com/drive/1QKOtYP7bMpdDJEipDxaJqOchv7oQ-_l\", visualize the results, save the data to a CSV file, and summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68de84d"
      },
      "source": [
        "## Inspect data\n",
        "\n",
        "### Subtask:\n",
        "Examine the extracted files to understand their format and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88714c44"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the file list and the common formats for biological data, the `.h5`, `.h5ad`, and `.csv` files are likely candidates for the relevant datasets. I will inspect the `.h5ad` file first as it's a common format for single-cell data and was mentioned in the Colab notebook, and then examine the `.csv` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd605e95"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `anndata` library is not installed. I need to install it before I can load the `.h5ad` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50af12db"
      },
      "source": [
        "**Reasoning**:\n",
        "The `anndata` library is now installed. I can proceed with loading and inspecting the `.h5ad` and `.csv` files to understand their format and content as planned in the original reasoning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdcznAMwlGJ8"
      },
      "outputs": [],
      "source": [
        "!pip install anndata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbou6Tdjzgrw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7759642"
      },
      "source": [
        "# Task\n",
        "Analyze the provided single-cell RNA sequencing data in the file \"competition_val_template.h5ad\" using the gene names from \"gene_names.csv\". Apply relevant preprocessing and analysis techniques based on the provided Colab notebooks, visualize the results, and save the analyzed data to a CSV file. Finally, summarize the key findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "572e6faf"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Load the `competition_val_template.h5ad` file into an AnnData object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b610b428"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to load the `competition_val_template.h5ad` file into an AnnData object as instructed. I have already installed the necessary library in a previous step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ab8979f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `anndata` library was not found, even though it was installed in a previous cell. I need to re-run the installation command and then attempt to load the data again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCsNDe8yyLc4"
      },
      "outputs": [],
      "source": [
        "!pip install anndata\n",
        "import anndata as ad\n",
        "import os\n",
        "\n",
        "# Load the .h5ad file into an AnnData object\n",
        "try:\n",
        "    # Assuming the file is inside the extracted directory 'competition_support_set'\n",
        "    adata = ad.read_h5ad('competition_support_set/competition_val_template.h5ad')\n",
        "    print(\"competition_val_template.h5ad loaded successfully.\")\n",
        "    # Print the AnnData object\n",
        "    print(adata)\n",
        "except FileNotFoundError:\n",
        "    print(\"competition_val_template.h5ad not found. Please ensure the data is extracted and in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading competition_val_template.h5ad: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc89976c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the file 'competition_val_template.h5ad' was not found. I need to verify the file name and path. I will list the files in the current directory again to confirm the correct filename.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTKhZ580yRrL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62dd9d59"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command showed that the extracted files are not in the current directory. This is likely because the extraction happened in a different location. I need to re-extract the zip file to the current directory and then attempt to load the .h5ad file again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b2dd78"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the zip file `competition_support_set.zip` was not found. I need to verify the file name and path for the zip file. I will list the files in the root directory and the current directory to confirm the correct filename and location of the zip file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHWGulLHyVb0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"Files in current directory:\", os.listdir('.'))\n",
        "print(\"Files in root directory:\", os.listdir('/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "427726d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The output of the previous command shows that the `competition_support_set.zip` file is not in the current directory or the root directory. It seems the download might not have completed successfully or was saved in a different location. Based on the initial notebook structure and the previous outputs, it's likely the file was intended to be in the `/content` directory, which is the typical working directory in Colab environments. I will try to locate the zip file in `/content` and if found, re-extract it to the current working directory (`.`), then load the .h5ad file. If not found in `/content`, I will indicate failure as I cannot proceed without the data file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31e38550"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import anndata as ad\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/competition_support_set.zip'\n",
        "\n",
        "if os.path.exists(zip_file_path):\n",
        "    print(f\"Found zip file at: {zip_file_path}\")\n",
        "    # Re-extract the zip file to the current directory\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    print(\"Zip file extracted.\")\n",
        "\n",
        "    # Load the .h5ad file into an AnnData object\n",
        "    try:\n",
        "        adata = ad.read_h5ad('competition_val_template.h5ad')\n",
        "        print(\"competition_val_template.h5ad loaded successfully.\")\n",
        "        # Print the AnnData object\n",
        "        print(adata)\n",
        "    except FileNotFoundError:\n",
        "        print(\"competition_val_template.h5ad not found after extraction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading competition_val_template.h5ad: {e}\")\n",
        "else:\n",
        "    print(f\"Zip file not found at: {zip_file_path}\")\n",
        "    print(\"Cannot proceed with loading the .h5ad file as the required data is missing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934c98d0"
      },
      "source": [
        "## Load gene names\n",
        "\n",
        "### Subtask:\n",
        "Load the `gene_names.csv` file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8825d8f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the `gene_names.csv` file into a pandas DataFrame and display the head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fe0c88"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because 'gene_names.csv' was not found. I need to list the files in the current directory to confirm the filename and location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSxhhNNOycZB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e66444"
      },
      "source": [
        "**Reasoning**:\n",
        "The file 'gene_names.csv' is not in the current directory, as confirmed by the previous output. The file list from the extraction step showed that the files were extracted to the current directory previously. It seems the environment reset and the extracted files are no longer present. I need to re-download and re-extract the data to access `gene_names.csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bd3d9fe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "url = \"https://storage.googleapis.com/vcc_data_prod/datasets/state/competition_support_set.zip\"\n",
        "output_path = \"competition_support_set.zip\"\n",
        "\n",
        "# Stream the download\n",
        "response = requests.get(url, stream=True)\n",
        "total = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "with open(output_path, \"wb\") as f, tqdm(\n",
        "    total=total, unit='B', unit_scale=True, desc=\"Downloading\"\n",
        ") as bar:\n",
        "    for chunk in response.iter_content(chunk_size=8192):\n",
        "        if not chunk:\n",
        "            break\n",
        "        f.write(chunk)\n",
        "        bar.update(len(chunk))\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile('competition_support_set.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "# List files to confirm extraction\n",
        "print(\"Files in the extracted directory:\")\n",
        "for file in os.listdir('.'):\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db2sb5rAzmf0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fde47a7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command successfully downloaded and extracted the zip file, and the file listing confirms that `gene_names.csv` is now in the current directory. I can now proceed to load the `gene_names.csv` file into a pandas DataFrame and display its head as originally planned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0957c8d3"
      },
      "outputs": [],
      "source": [
        "gene_names_df = pd.read_csv('gene_names.csv')\n",
        "display(gene_names_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc3ef4cf"
      },
      "source": [
        "## Preprocessing and analysis\n",
        "\n",
        "### Subtask:\n",
        "Apply relevant preprocessing steps and analysis based on the provided Colab notebooks. This may include normalization, dimensionality reduction (e.g., PCA, UMAP), clustering, and differential expression analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18f78ead"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now attempt to load the `competition_val_template.h5ad` file, as the required data has been confirmed to be extracted in the previous subtask. This is a necessary first step before applying any preprocessing or analysis based on the Colab notebooks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0c0949c"
      },
      "outputs": [],
      "source": [
        "import anndata as ad\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    adata = ad.read_h5ad('competition_val_template.h5ad')\n",
        "    print(\"competition_val_template.h5ad loaded successfully.\")\n",
        "    print(adata)\n",
        "except FileNotFoundError:\n",
        "    print(\"competition_val_template.h5ad not found. Please ensure the data is extracted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading competition_val_template.h5ad: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7db840"
      },
      "source": [
        "**Reasoning**:\n",
        "The `competition_val_template.h5ad` file has been successfully loaded into the `adata` object. The next steps, based on common single-cell RNA sequencing analysis workflows and the likely content of the provided Colab notebooks, involve preprocessing the data. This typically includes making observation names unique (as suggested by the warning), filtering low-quality cells and genes, normalizing the data, and potentially scaling. I will perform these initial preprocessing steps within a single code block using the `scanpy` library, which is commonly used for AnnData objects and aligns with the likely tools used in the Colab notebooks. I will install `scanpy` first as it is not yet installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMcUtxqe0oeT"
      },
      "outputs": [],
      "source": [
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee34f530"
      },
      "source": [
        "**Reasoning**:\n",
        "`scanpy` has been successfully installed. Now I will perform the initial preprocessing steps on the loaded AnnData object `adata` using `scanpy`, including making observation names unique, filtering, normalization, and scaling, which are standard practices in single-cell RNA-seq analysis and likely part of the referenced Colab notebooks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc91699"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed with an `ImportError` related to `scipy`. This indicates an incompatibility issue between the installed versions of `scanpy` and `scipy`, likely due to the `scipy` version being too new for the `scanpy` version that was installed. I will try installing a specific version of `scipy` that is known to be compatible with the installed `scanpy` version (1.11.3), such as version 1.13.0, and then re-attempt the preprocessing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph14AEqz0y3p"
      },
      "outputs": [],
      "source": [
        "!pip install scipy==1.13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5f7879"
      },
      "source": [
        "**Reasoning**:\n",
        "I have installed a potentially compatible version of `scipy`. I will now re-attempt the preprocessing steps using `scanpy` on the loaded `adata` object. This includes making observation names unique, filtering cells and genes, normalizing and logarithmizing the data, identifying highly variable genes, and scaling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba82132"
      },
      "source": [
        "# Task\n",
        "Analyze the provided single-cell RNA sequencing data from \"competition_val_template.h5ad\" and gene names from \"gene_names.csv\". Clean and preprocess the data, perform statistical analysis including mean, median, mode, IQR, and ANOVA. Apply dimensionality reduction using PCA and UMAP, and perform clustering. Prepare the data for a Convolutional Neural Network (CNN), build and train a TensorFlow CNN model using the training and validation datasets, incorporating bootstrapping. Evaluate the model using MAE. Compute intercept and coefficients if applicable, and implement gradient descent if necessary. Save relevant data to a CSV file and summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae280683"
      },
      "source": [
        "## Data cleaning and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Apply relevant preprocessing steps and analysis based on the provided Colab notebooks. This may include normalization, dimensionality reduction (e.g., PCA, UMAP), clustering, and differential expression analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "709b7e46"
      },
      "source": [
        "## Data cleaning and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Apply relevant preprocessing steps and analysis based on the provided Colab notebooks. This may include normalization, dimensionality reduction (e.g., PCA, UMAP), clustering, and differential expression analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43da02da"
      },
      "source": [
        "## Statistical analysis\n",
        "\n",
        "### Subtask:\n",
        "Compute mean, median, mode, and Interquartile range for relevant features. Perform ANOVA test to compare groups if applicable based on the data structure (e.g., cell types, treatments).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07326dd5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous preprocessing steps failed due to library incompatibility issues. Therefore, I cannot compute summary statistics on the processed gene expression data as originally intended. However, I can still analyze the categorical data available in `adata.obs`. I will compute the mode for the 'cell_type' and 'target_gene' columns and then assess the applicability of an ANOVA test based on the available data structure, explaining why it cannot be performed without the preprocessed numerical data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4786fe5a"
      },
      "source": [
        "## Dimensionality reduction\n",
        "\n",
        "### Subtask:\n",
        "Apply PCA for dimensionality reduction on the preprocessed AnnData object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd19cf07"
      },
      "source": [
        "**Reasoning**:\n",
        "I will attempt to apply PCA using `scanpy` now that a compatible version of `scipy` has been installed. This aligns with the subtask of applying dimensionality reduction. I will import `scanpy` and apply `sc.pp.pca()` to the `adata` object, then print the object to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111c67f3"
      },
      "source": [
        "## Dimensionality reduction\n",
        "\n",
        "### Subtask:\n",
        "Apply UMAP for dimensionality reduction and visualization on the preprocessed AnnData object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd464ebf"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to apply UMAP for dimensionality reduction and visualization. This requires computing neighbors and then performing UMAP. I will use `scanpy` for these steps. The previous attempts to use scanpy failed due to scipy incompatibility, but since the environment might have changed or been reset, I will try again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1402fcab"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "### Subtask:\n",
        "Perform clustering on the reduced dimensions (e.g., using Leiden or Louvain algorithm) and visualize the clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de2d997"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to perform clustering using the Leiden algorithm on the UMAP reduced dimensions and visualize the clusters. I will use `scanpy` for this, assuming the previous dependency issues have been resolved or are not present in this new execution context. I will first compute the neighborhood graph, then run the Leiden clustering algorithm, and finally generate a UMAP plot colored by the resulting clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db288d5f"
      },
      "source": [
        "## Prepare data for cnn\n",
        "\n",
        "### Subtask:\n",
        "Restructure the data into a format suitable for a Convolutional Neural Network (CNN) and split it into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6548f88f"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the raw data from `adata.X` and the target variable `adata.obs['target_gene']` for CNN input by converting them to NumPy arrays and splitting them into training and validation sets. This step is necessary to fulfill the current subtask requirements, acknowledging that previous preprocessing and dimensionality reduction steps failed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afc73cc1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Attempts to perform preprocessing steps like filtering, normalization, log transformation, identification of highly variable genes, and scaling using `scanpy` failed due to persistent `ImportError` related to the `scipy` library, specifically the inability to import `_lazywhere`.\n",
        "*   Statistical analysis, including calculating mean, median, mode, IQR, and performing an ANOVA test, could not be completed. The `scipy` import error occurred immediately when trying to import necessary statistical functions like `iqr` and `f_oneway`.\n",
        "*   Dimensionality reduction using PCA and UMAP was attempted but failed because the `scanpy` library could not be successfully imported due to the `scipy` dependency conflict.\n",
        "*   Clustering using the Leiden algorithm and visualization of clusters on a UMAP plot could not be performed as these steps require a functional `scanpy` environment, which was unavailable due to the `scipy` issue.\n",
        "*   Preparing data for a CNN by splitting features and target variables into training and validation sets using `sklearn.model_selection.train_test_split` also failed. The `ImportError` related to `scipy` reoccurred during the import of `train_test_split`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary obstacle to completing the task is a fundamental and persistent library dependency conflict involving `scipy`. This issue needs to be resolved at the environment level before any data analysis or modeling steps can be successfully executed.\n",
        "*   Future attempts should focus on ensuring a compatible installation of `scipy` with all required libraries (`scanpy`, `sklearn`) before proceeding with any data processing or analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9845f1af"
      },
      "outputs": [],
      "source": [
        "!pip install anndata\n",
        "import anndata as ad\n",
        "\n",
        "# Load the .h5ad file into an AnnData object\n",
        "try:\n",
        "    adata = ad.read_h5ad('competition_val_template.h5ad')\n",
        "    print(\"competition_val_template.h5ad loaded successfully.\")\n",
        "    # Print the AnnData object\n",
        "    print(adata)\n",
        "except FileNotFoundError:\n",
        "    print(\"competition_val_template.h5ad not found. Please ensure the data is extracted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading competition_val_template.h5ad: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159eda5f"
      },
      "source": [
        "## Load gene names\n",
        "\n",
        "### Subtask:\n",
        "Load the `gene_names.csv` file into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1feaa4da"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the `gene_names.csv` file into a pandas DataFrame and display the head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06702a55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    gene_names_df = pd.read_csv('competition_support_set/gene_names.csv')\n",
        "    print(\"gene_names.csv loaded successfully.\")\n",
        "    display(gene_names_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"gene_names.csv not found. Please ensure the data is extracted and in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading gene_names.csv: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Statistical analysis"
      ],
      "metadata": {
        "id": "mdMN0jVWjeAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning, Outliers Handling and  Statistical Analysis using ANOVA(Hypothesis testing), Kmean Clustering, Bootstrapping and PCA\n",
        "\n",
        "# Data Cleaning\n",
        "\n",
        "# Display the first five rows\n",
        "# print(training_data.head())\n",
        "# print(validation_data.head())\n",
        "# print(output_path_data.head())\n",
        "# print(training_data.describe())\n",
        "\n",
        "# check for missing values\n",
        "# print(training_data.isnull().sum())\n",
        "# print(validation_data. isnull().sum())\n",
        "\n",
        "\n",
        "# Test of Hypothesis (ANOVA)\n",
        "# used to test the significant difference  in the means of the datasets using scipy.stats.f_oneway\n",
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "# synthetic data\n",
        "group1  = np.random.normal(10, 2, 30)\n",
        "group2 = np.random.normal(12, 2, 30)\n",
        "group3 = np.random.normal(14, 2, 30) # Corrected syntax\n",
        "F_stat, p_val = f_oneway(group1, group2, group3)\n",
        "print(f\"F_statistics:{F_stat:.2f}, p_value: {p_val:.4f}\") # Corrected f-string syntax\n",
        "\n",
        "# Cluster Analysis\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans # Corrected class name\n",
        "# synthetic data  target_genes counts,  n_cells, umi\n",
        "x = np.random.rand(100, 2) # Corrected synthetic data generation\n",
        "kmeans = KMeans(n_clusters=3, random_state = 42, n_init=10) # Corrected class name and added n_init\n",
        "kmeans.fit(x)\n",
        "print(\"cluster centers: \", kmeans.cluster_centers_)\n",
        "print(\"cluster labels:\", kmeans.labels_[: 10])\n",
        "\n",
        "# Bootstrapping analysis\n",
        "# confidence interval for mean of the smallest dataset at 95%\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(50, 5, size=100)# original sample\n",
        "def bootstrap_mean_ci(data, n_bootstraps=1000, ci=95):\n",
        "    means = []\n",
        "    n = len(data)\n",
        "    for _ in range(n_bootstraps): # Corrected syntax\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means.append(np.mean(sample))\n",
        "    lower = np.percentile(means, (100-ci)/2) # Corrected calculation\n",
        "    upper = np.percentile(means, 100-(100-ci)/2) # Corrected calculation\n",
        "    return np.mean(means),(lower,upper) # Corrected return statement\n",
        "\n",
        "# Example usage of bootstrap_mean_ci\n",
        "mean, ci_interval = bootstrap_mean_ci(data)\n",
        "print(f\"Bootstrap mean: {mean:.2f}, 95% Confidence Interval: {ci_interval}\")\n",
        "\n",
        "\n",
        "# PCA Analysis\n",
        "# reduce dimensionality of data by projecting it unto new orthogonal axes capturing the most variance image compression analysis\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "# synthetic data: training and validation target genes and cell type counts with 10 features\n",
        "x = np.random.rand(100, 10) # Corrected synthetic data generation\n",
        "pca = PCA(n_components=2)\n",
        "x_reduced = pca.fit_transform(x)\n",
        "print(\"PCA reduced data shape:\", x_reduced.shape)"
      ],
      "metadata": {
        "id": "qluc5Wj9jume"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming x_reduced is available from the previous PCA step\n",
        "if 'x_reduced' in locals():\n",
        "    # Create a DataFrame for easier plotting with pandas\n",
        "    pca_df = pd.DataFrame(x_reduced, columns=[f'PC{i+1}' for i in range(x_reduced.shape[1])])\n",
        "\n",
        "    # Generate boxplots for each principal component\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pca_df.boxplot()\n",
        "    plt.title('Boxplot of PCA Reduced Data')\n",
        "    plt.ylabel('Value')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"PCA reduced data (x_reduced) not found. Please run the PCA analysis cell first.\")"
      ],
      "metadata": {
        "id": "9PknT8welIKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the CNN"
      ],
      "metadata": {
        "id": "ByOjPVk3kWbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Download dataset\n",
        "url = \"https://storage.googleapis.com/vcc_data_prod/datasets/state/competition_support_set.zip\"\n",
        "output_path = \"competition_support_set.zip\"\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "total = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "with open(output_path, \"wb\") as f, tqdm(\n",
        "    total=total, unit='B', unit_scale=True, desc=\"Downloading\"\n",
        ") as bar:\n",
        "    for chunk in response.iter_content(chunk_size=8192):\n",
        "        if chunk:\n",
        "            f.write(chunk)\n",
        "            bar.update(len(chunk))\n",
        "\n",
        "# Unzip dataset\n",
        "extracted_dir = \"competition_support_set\"\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "# Set up paths\n",
        "base_dir = extracted_dir\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir = os.path.join(base_dir, \"validation\")\n",
        "\n",
        "# Create train and validation directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Move extracted files to train and validation directories\n",
        "# Assuming the zip file contains train and validation data directly at the top level\n",
        "for item in os.listdir(base_dir):\n",
        "    item_path = os.path.join(base_dir, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        # Assuming subdirectories in the zip correspond to classes\n",
        "        # and need to be moved into train/validation based on some criteria\n",
        "        # This part might need adjustment based on the actual zip file structure\n",
        "        # For now, let's assume all directories are data for training\n",
        "        # and we need a way to split them.\n",
        "        # Since the original plan mentioned train and validation sets,\n",
        "        # and the error was looking for train/validation directories,\n",
        "        # let's assume the zip contains train and validation folders directly.\n",
        "        # If not, this logic needs to be updated based on the actual content.\n",
        "\n",
        "        # Let's assume the zip contains 'train' and 'validation' folders\n",
        "        # and we just need to move their contents if they exist directly in extracted_dir\n",
        "        if item == 'train':\n",
        "             for filename in os.listdir(item_path):\n",
        "                 shutil.move(os.path.join(item_path, filename), train_dir)\n",
        "        elif item == 'validation':\n",
        "             for filename in os.listdir(item_path):\n",
        "                 shutil.move(os.path.join(item_path, filename), val_dir)\n",
        "        else:\n",
        "            # If there are other directories, you might need specific logic\n",
        "            pass\n",
        "    elif os.path.isfile(item_path):\n",
        "        # Handle individual files if necessary\n",
        "        pass\n",
        "\n",
        "\n",
        "# Step 2: Data Loading and Preprocessing\n",
        "# Image parameters\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data generators with augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load datasets\n",
        "# Check if train and validation directories are populated\n",
        "if not os.listdir(train_dir):\n",
        "    print(f\"Error: '{train_dir}' is empty. Please check the zip file structure and extraction logic.\")\n",
        "elif not os.listdir(val_dir):\n",
        "    print(f\"Error: '{val_dir}' is empty. Please check the zip file structure and extraction logic.\")\n",
        "else:\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # Step 3: Build the CNN Architecture\n",
        "    def build_cnn(num_classes):\n",
        "        model = models.Sequential([\n",
        "            # Convolutional Base\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "            # Classifier\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    num_classes = len(train_generator.class_indices)\n",
        "    model = build_cnn(num_classes)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
        "\n",
        "    # Step 4: Train with Callbacks\n",
        "    # Callbacks\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=40000,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "\n",
        "\n",
        "    # Step 5: Evaluate and Fine-Tune\n",
        "    # Evaluate final model\n",
        "    val_loss, val_acc, val_mae = model.evaluate(val_generator)\n",
        "    print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "    # If accuracy < 98%, add transfer learning\n",
        "    if val_acc < 0.98:\n",
        "        base_model = tf.keras.applications.EfficientNetB0(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(128, 128, 3)\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        new_model = models.Sequential([\n",
        "            base_model,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        new_model.compile(optimizer='adam',\n",
        "                          loss='categorical_crossentropy',\n",
        "                          metrics=['accuracy', 'mae'])\n",
        "\n",
        "        history = new_model.fit(\n",
        "            train_generator,\n",
        "            epochs=40000,\n",
        "            validation_data=val_generator,\n",
        "            callbacks=[early_stopping, reduce_lr]\n",
        "        )\n",
        "\n",
        "        # Fine-tune\n",
        "        base_model.trainable = True\n",
        "        for layer in base_model.layers[:100]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        new_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                          loss='categorical_crossentropy',\n",
        "                          metrics=['accuracy', 'mae'])\n",
        "\n",
        "        history = new_model.fit(\n",
        "            train_generator,\n",
        "            epochs=40000,\n",
        "            validation_data=val_generator,\n",
        "            callbacks=[early_stopping]\n",
        "        )\n",
        "\n",
        "        # Evaluate final model\n",
        "        val_loss, val_acc, val_mae = new_model.evaluate(val_generator)\n",
        "        print(f\"\\nFinal Validation Accuracy: {val_acc:.4f}\")\n",
        "        print(f\"Final Validation MAE: {val_mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "67a9ef5ab25440ba96f20ac3465d2ffa",
            "aba322b7de074393acecbd82c6d3c908",
            "bc008ff1e0bc47faaf6f9e95dd7c3238",
            "902e747447c74632a12c2fab884bc601",
            "0f617b3ee47f4aa5991bfa7038b4ea2c",
            "237184a90bb94a04b4cc472b3945847a",
            "ef2802c74c7449f7931ef6ae96511db6",
            "402d82c75fca46c1955c1907f43f71e4",
            "e6dec2c804b54b4d8550e2d90f1bf9c3",
            "739102c1240f440fa1d00fa97ea62c8f",
            "ecc570e11c584721aa5fa82d7170b728"
          ]
        },
        "id": "gcqnimUqkoKW",
        "outputId": "8c73f36a-c166-41ea-f272-4599609c17d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/6.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67a9ef5ab25440ba96f20ac3465d2ffa"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAsU+SkFpGWNpNzV++sblr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67a9ef5ab25440ba96f20ac3465d2ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aba322b7de074393acecbd82c6d3c908",
              "IPY_MODEL_bc008ff1e0bc47faaf6f9e95dd7c3238",
              "IPY_MODEL_902e747447c74632a12c2fab884bc601"
            ],
            "layout": "IPY_MODEL_0f617b3ee47f4aa5991bfa7038b4ea2c"
          }
        },
        "aba322b7de074393acecbd82c6d3c908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237184a90bb94a04b4cc472b3945847a",
            "placeholder": "​",
            "style": "IPY_MODEL_ef2802c74c7449f7931ef6ae96511db6",
            "value": "Downloading: 100%"
          }
        },
        "bc008ff1e0bc47faaf6f9e95dd7c3238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_402d82c75fca46c1955c1907f43f71e4",
            "max": 6688587887,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6dec2c804b54b4d8550e2d90f1bf9c3",
            "value": 6688587887
          }
        },
        "902e747447c74632a12c2fab884bc601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739102c1240f440fa1d00fa97ea62c8f",
            "placeholder": "​",
            "style": "IPY_MODEL_ecc570e11c584721aa5fa82d7170b728",
            "value": " 6.69G/6.69G [02:44&lt;00:00, 60.7MB/s]"
          }
        },
        "0f617b3ee47f4aa5991bfa7038b4ea2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237184a90bb94a04b4cc472b3945847a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef2802c74c7449f7931ef6ae96511db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "402d82c75fca46c1955c1907f43f71e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6dec2c804b54b4d8550e2d90f1bf9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "739102c1240f440fa1d00fa97ea62c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc570e11c584721aa5fa82d7170b728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}